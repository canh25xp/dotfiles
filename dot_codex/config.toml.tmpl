model = "gpt-5-codex"
model_reasoning_effort = "medium"

sandbox_mode = "workspace-write"
approval_policy = "on-request"

[features]
web_search_request = true

# ==============================================================================
# [PROVIDERS]
# ==============================================================================

[model_providers.openai-chat-completions]
# Name of the provider that will be displayed in the Codex UI.
name = "OpenAI using Chat Completions"
# The path `/chat/completions` will be amended to this URL to make the POST
# request for the chat completions.
base_url = "https://api.openai.com/v1"
# If `env_key` is set, identifies an environment variable that must be set when
# using Codex with this provider. The value of the environment variable must be
# non-empty and will be used in the `Bearer TOKEN` HTTP header for the POST request.
env_key = "OPENAI_API_KEY"
# Valid values for wire_api are "chat" and "responses". Defaults to "chat" if omitted.
wire_api = "responses"
# If necessary, extra query params that need to be added to the URL.
# See the Azure example below.
query_params = {}

# Use local ollama as provider.
# NOTE: This is kinda redundant since codex already support `--oss` flag
# `codex --oss`                       to use ollama local which use gpt-oss:20b by default
# `codex --oss -m gpt-oss:120b-cloud` to use ollama cloud model gpt-oss:120b
[model_providers.ollama]
name = "Ollama local"
base_url = "http://localhost:11434/v1"
wire_api = "responses"
request_max_retries = 1 # It's a local model, there's not really a reason to fail unless ollama really did not running.
stream_max_retries = 3

# Use ollama.com as provider. Require OLLAMA_API_KEY environment
# NOTE: You can access the same cloud model by just apending `:cloud` to ollama provider
[model_providers.ollama-cloud]
name = "Ollama cloud"
base_url = "https://ollama.com/v1"
env_key = "OLLAMA_API_KEY"

# ==============================================================================
# [PROFILES]
# ==============================================================================

[profiles.gpt-oss]
model_provider = "ollama"
model = "gpt-oss:20b"
model_reasoning_effort = "medium"

[profiles.gemini-3-flash]
model_provider = "ollama"
model = "gemini-3-flash-preview:cloud"
model_reasoning_effort = "medium"

[profiles.gpt-oss-cloud]
model_provider = "ollama-cloud"
model = "gpt-oss:120b"
model_reasoning_effort = "medium"

# ==============================================================================
# [MCP]
# ==============================================================================

[mcp_servers.context7]
url = "https://mcp.context7.com/mcp"
env_http_headers = { "CONTEXT7_API_KEY" = "CONTEXT7_API_KEY" }

# ==============================================================================
# [PROJECTS]
# ==============================================================================

[projects.{{ joinPath .chezmoi.homeDir ".local/share/chezmoi" | quote }}]
trust_level = "trusted"

[projects.{{ joinPath .chezmoi.homeDir ".config/nvim-lazy" | quote }}]
trust_level = "trusted"
